{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance Gap Metrics\n",
    "\n",
    "**Category Focus:** *\"Measuring specific types of errors\"*\n",
    "\n",
    "This notebook explores Performance Gap Metrics that identify exactly where your model makes unfair mistakes, enabling targeted fixes. These metrics help diagnose and fix **specific bias problems** by measuring differences in error rates across demographic groups.\n",
    "\n",
    "## Metrics in This Category\n",
    "\n",
    "1. **FNR Difference** - Missing qualified candidates (talent loss)\n",
    "2. **FOR Difference** - Wrongly rejecting people (false accusations)\n",
    "3. **FPR Difference** - Wrongly approving people (risk exposure)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìö Descriptive Analysis\n",
    "\n",
    "### False Negative Rate (FNR) Difference\n",
    "**Definition:** Measures differences in missing qualified candidates across demographic groups.\n",
    "\n",
    "**Formula:** |FNR_group1 - FNR_group2|\n",
    "- Where FNR = False Negatives / (True Positives + False Negatives)\n",
    "- FNR = 1 - True Positive Rate (TPR)\n",
    "\n",
    "**Business Meaning:** \n",
    "- Identifies if the model systematically misses qualified candidates from specific groups\n",
    "- High FNR difference = talent loss from underrepresented groups\n",
    "- Directly impacts diversity in hiring and opportunity access\n",
    "\n",
    "**Diagnostic Value:**\n",
    "- Reveals hidden bias in qualification recognition\n",
    "- Shows where model training data may be incomplete\n",
    "- Indicates need for threshold adjustment or feature engineering\n",
    "\n",
    "**When to Prioritize:**\n",
    "- Talent acquisition where missing good candidates is costly\n",
    "- Medical diagnosis where false negatives have serious consequences\n",
    "- Any domain where opportunity access is critical\n",
    "\n",
    "---\n",
    "\n",
    "### False Omission Rate (FOR) Difference\n",
    "**Definition:** Measures differences in wrongly rejecting people across demographic groups.\n",
    "\n",
    "**Formula:** |FOR_group1 - FOR_group2|\n",
    "- Where FOR = False Negatives / (True Negatives + False Negatives)\n",
    "- FOR = Proportion of false negatives among all negative predictions\n",
    "\n",
    "**Business Meaning:**\n",
    "- Identifies if the model systematically under-predicts positive outcomes for specific groups\n",
    "- High FOR difference = unfair rejection rates leading to missed opportunities\n",
    "- Measures reliability of negative predictions across groups\n",
    "\n",
    "**Diagnostic Value:**\n",
    "- Shows calibration problems in prediction confidence\n",
    "- Reveals when model is overly conservative for certain groups\n",
    "- Indicates need for group-specific thresholds\n",
    "\n",
    "**When to Prioritize:**\n",
    "- Loan approvals where false rejections harm access to credit\n",
    "- Educational opportunities where false rejections limit advancement\n",
    "- Any system where rejection has long-term consequences\n",
    "\n",
    "---\n",
    "\n",
    "### False Positive Rate (FPR) Difference\n",
    "**Definition:** Measures differences in wrongly approving people across demographic groups.\n",
    "\n",
    "**Formula:** |FPR_group1 - FPR_group2|\n",
    "- Where FPR = False Positives / (True Negatives + False Positives)\n",
    "- FPR = 1 - True Negative Rate (TNR)\n",
    "\n",
    "**Business Meaning:**\n",
    "- Identifies if the model systematically over-predicts positive outcomes for specific groups\n",
    "- High FPR difference = unfair advantage or increased risk exposure\n",
    "- Measures consistency of risk assessment across groups\n",
    "\n",
    "**Diagnostic Value:**\n",
    "- Reveals when model is overly optimistic for certain groups\n",
    "- Shows potential bias in training data representation\n",
    "- Indicates need for improved negative case recognition\n",
    "\n",
    "**When to Prioritize:**\n",
    "- Credit scoring where false positives increase default risk\n",
    "- Security systems where false alarms waste resources\n",
    "- Medical screening where false positives cause unnecessary treatment\n",
    "\n",
    "---\n",
    "\n",
    "## üîß Diagnostic Framework\n",
    "\n",
    "| Error Type | What It Reveals | Fix Strategy |\n",
    "|------------|-----------------|-------------|\n",
    "| **High FNR Diff** | Missing qualified people from group X | Improve training data, lower thresholds for group X |\n",
    "| **High FOR Diff** | Poor negative prediction reliability for group X | Calibrate confidence scores, adjust rejection criteria |\n",
    "| **High FPR Diff** | Too many false approvals for group X | Tighten approval criteria, improve feature representation |\n",
    "\n",
    "**Key Insight:** Performance Gap metrics are **diagnostic tools** - they tell you exactly what type of bias problem you have so you can fix it systematically.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üíª Computational Analysis\n",
    "\n",
    "Let's implement and analyze these three performance gap metrics using the Adult Income dataset to diagnose specific bias problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from jurity.fairness import BinaryFairnessMetrics\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set visualization style\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"Libraries imported successfully!\")\n",
    "print(\"Ready to diagnose performance gaps:\")\n",
    "print(\"‚Ä¢ FNR Difference (Missing Talent)\")\n",
    "print(\"‚Ä¢ FOR Difference (False Rejections)\")\n",
    "print(\"‚Ä¢ FPR Difference (False Approvals)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Load and prepare the Adult Income dataset\nprint(\"=== LOADING DIAGNOSTIC DATASET ===\")\nprint(\"Context: Performance gap analysis for bias detection\")\n\nurl = \"https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data\"\ncolumns = ['age', 'workclass', 'fnlwgt', 'education', 'education_num', 'marital_status',\n          'occupation', 'relationship', 'race', 'sex', 'capital_gain', 'capital_loss',\n          'hours_per_week', 'native_country', 'income']\n\ndata = pd.read_csv(url, names=columns, skipinitialspace=True)\n\nprint(f\"Dataset shape: {data.shape}\")\nprint(\"\\nPerformance gap analysis focuses on:\")\nprint(\"‚Ä¢ False Negative Rate differences (missing qualified people)\")\nprint(\"‚Ä¢ False Omission Rate differences (wrong rejections)\")\nprint(\"‚Ä¢ False Positive Rate differences (wrong approvals)\")\n\nprint(\"\\nTarget and sensitive attribute distribution:\")\nprint(f\"Income: {data['income'].value_counts().to_dict()}\")\nprint(f\"Gender: {data['sex'].value_counts().to_dict()}\")\n\n# Prepare features and target\nprint(\"\\n=== PREPARING FEATURES ===\")\n\n# Create target variable (1 for >50K, 0 for <=50K)\ny = (data['income'] == '>50K').astype(int)\n\n# Create sensitive attribute (1 for Female, 0 for Male)\nsensitive_attribute = (data['sex'] == 'Female').astype(int)\n\n# Select and encode features\ncategorical_features = ['workclass', 'education', 'marital_status', 'occupation', \n                       'relationship', 'race', 'native_country']\nnumerical_features = ['age', 'education_num', 'capital_gain', 'capital_loss', 'hours_per_week']\n\n# Encode categorical features\nX = data[numerical_features].copy()\nle = LabelEncoder()\nfor col in categorical_features:\n    X[col] = le.fit_transform(data[col].astype(str))\n\nprint(f\"Features prepared: {X.shape[1]} features, {X.shape[0]} samples\")\nprint(f\"Target distribution: {y.value_counts().to_dict()}\")\nprint(f\"Sensitive attribute: {sensitive_attribute.value_counts().to_dict()}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data and train model\n",
    "X_train, X_test, y_train, y_test, sensitive_train, sensitive_test = train_test_split(\n",
    "    X, y, sensitive_attribute, test_size=0.3, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Train Random Forest model\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = rf_model.predict(X_test)\n",
    "y_pred_proba = rf_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "print(\"=== MODEL PERFORMANCE ===\")\n",
    "print(f\"Test set accuracy: {(y_pred == y_test).mean():.3f}\")\n",
    "print(f\"Test set size: {len(y_test)}\")\n",
    "\n",
    "# Show basic prediction statistics by group\n",
    "female_mask = sensitive_test == 1\n",
    "male_mask = sensitive_test == 0\n",
    "\n",
    "female_predictions = y_pred[female_mask]\n",
    "male_predictions = y_pred[male_mask]\n",
    "\n",
    "print(f\"\\nPrediction rates by group:\")\n",
    "print(f\"Female: {female_predictions.mean():.3f} ({female_predictions.sum()} of {len(female_predictions)})\")\n",
    "print(f\"Male: {male_predictions.mean():.3f} ({male_predictions.sum()} of {len(male_predictions)})\")\n",
    "\n",
    "print(\"\\nüîç Ready for detailed performance gap analysis...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Calculate Performance Gap Metrics using Jurity\nprint(\"=== PERFORMANCE GAP METRICS ANALYSIS ===\")\n\nbfm = BinaryFairnessMetrics()\n\n# FNR Difference (False Negative Rate) - Available in Jurity\nfnr_difference = bfm.FNRDifference.get_score(\n    labels=y_test.values,\n    predictions=y_pred,\n    memberships=sensitive_test.values\n)\n\n# FOR Difference (False Omission Rate) - Available in Jurity\nfor_difference = bfm.FORDifference.get_score(\n    labels=y_test.values,\n    predictions=y_pred,\n    memberships=sensitive_test.values\n)\n\n# FPR Difference (False Positive Rate) - Calculate manually (not available in Jurity)\ndef calculate_fpr_difference(y_true, y_pred, sensitive):\n    \"\"\"Calculate False Positive Rate difference manually\"\"\"\n    results = {}\n    for group in [0, 1]:  # 0=Male, 1=Female\n        mask = sensitive == group\n        y_true_group = y_true[mask]\n        y_pred_group = y_pred[mask]\n        \n        tn = ((y_true_group == 0) & (y_pred_group == 0)).sum()\n        fp = ((y_true_group == 0) & (y_pred_group == 1)).sum()\n        fpr = fp / (fp + tn) if (fp + tn) > 0 else 0\n        \n        group_name = 'Female' if group == 1 else 'Male'\n        results[group_name] = fpr\n    \n    return abs(results['Female'] - results['Male'])\n\nfpr_difference = calculate_fpr_difference(y_test.values, y_pred, sensitive_test.values)\n\nprint(\"üîß PERFORMANCE GAP RESULTS:\")\nprint(f\"FNR Difference: {fnr_difference:.4f} (Missing qualified candidates)\")\nprint(f\"FOR Difference: {for_difference:.4f} (False rejections)\")\nprint(f\"FPR Difference: {fpr_difference:.4f} (False approvals)\")\n\nprint(\"\\nüìä INTERPRETATION:\")\nprint(\"‚Ä¢ Higher FNR Difference = More qualified people missed from one group\")\nprint(\"‚Ä¢ Higher FOR Difference = More unreliable negative predictions for one group\")\nprint(\"‚Ä¢ Higher FPR Difference = More false positives for one group\")\nprint(\"\\nNote: Values closer to 0 indicate better fairness across error types\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed confusion matrix analysis by group\n",
    "def calculate_detailed_metrics_by_group(y_true, y_pred, sensitive):\n",
    "    \"\"\"Calculate detailed error metrics by sensitive group\"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    for group in [0, 1]:  # 0=Male, 1=Female\n",
    "        mask = sensitive == group\n",
    "        y_true_group = y_true[mask]\n",
    "        y_pred_group = y_pred[mask]\n",
    "        \n",
    "        # Confusion matrix components\n",
    "        tn = ((y_true_group == 0) & (y_pred_group == 0)).sum()\n",
    "        tp = ((y_true_group == 1) & (y_pred_group == 1)).sum()\n",
    "        fn = ((y_true_group == 1) & (y_pred_group == 0)).sum()\n",
    "        fp = ((y_true_group == 0) & (y_pred_group == 1)).sum()\n",
    "        \n",
    "        # Calculate error rates\n",
    "        fnr = fn / (tp + fn) if (tp + fn) > 0 else 0  # False Negative Rate\n",
    "        for_rate = fn / (tn + fn) if (tn + fn) > 0 else 0  # False Omission Rate\n",
    "        fpr = fp / (fp + tn) if (fp + tn) > 0 else 0  # False Positive Rate\n",
    "        \n",
    "        # Additional metrics for context\n",
    "        tpr = tp / (tp + fn) if (tp + fn) > 0 else 0  # True Positive Rate\n",
    "        tnr = tn / (tn + fp) if (tn + fp) > 0 else 0  # True Negative Rate\n",
    "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "        recall = tpr\n",
    "        \n",
    "        group_name = 'Female' if group == 1 else 'Male'\n",
    "        results[group_name] = {\n",
    "            'FNR': fnr, 'FOR': for_rate, 'FPR': fpr,\n",
    "            'TPR': tpr, 'TNR': tnr,\n",
    "            'Precision': precision, 'Recall': recall,\n",
    "            'TP': tp, 'TN': tn, 'FP': fp, 'FN': fn,\n",
    "            'Total': len(y_true_group),\n",
    "            'Positive_Cases': (y_true_group == 1).sum(),\n",
    "            'Negative_Cases': (y_true_group == 0).sum()\n",
    "        }\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Calculate detailed metrics\n",
    "group_metrics = calculate_detailed_metrics_by_group(y_test.values, y_pred, sensitive_test.values)\n",
    "\n",
    "print(\"=== DETAILED ERROR ANALYSIS BY GROUP ===\")\n",
    "for group, metrics in group_metrics.items():\n",
    "    print(f\"\\n{group.upper()} GROUP:\")\n",
    "    print(f\"  Sample size: {metrics['Total']} (Positive: {metrics['Positive_Cases']}, Negative: {metrics['Negative_Cases']})\")\n",
    "    print(f\"  \")\n",
    "    print(f\"  Error Rates (Performance Gaps):\")\n",
    "    print(f\"    False Negative Rate (FNR): {metrics['FNR']:.4f} (Missing qualified people)\")\n",
    "    print(f\"    False Omission Rate (FOR):  {metrics['FOR']:.4f} (Wrong rejection rate)\")\n",
    "    print(f\"    False Positive Rate (FPR):  {metrics['FPR']:.4f} (Wrong approval rate)\")\n",
    "    print(f\"  \")\n",
    "    print(f\"  Context Metrics:\")\n",
    "    print(f\"    True Positive Rate (TPR):   {metrics['TPR']:.4f} (Sensitivity/Recall)\")\n",
    "    print(f\"    True Negative Rate (TNR):   {metrics['TNR']:.4f} (Specificity)\")\n",
    "    print(f\"    Precision:                  {metrics['Precision']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manual verification of performance gap calculations\n",
    "male_metrics = group_metrics['Male']\n",
    "female_metrics = group_metrics['Female']\n",
    "\n",
    "# Calculate differences manually\n",
    "manual_fnr_diff = abs(female_metrics['FNR'] - male_metrics['FNR'])\n",
    "manual_for_diff = abs(female_metrics['FOR'] - male_metrics['FOR'])\n",
    "manual_fpr_diff = abs(female_metrics['FPR'] - male_metrics['FPR'])\n",
    "\n",
    "print(\"=== MANUAL VERIFICATION OF PERFORMANCE GAP METRICS ===\")\n",
    "\n",
    "print(f\"\\nüéØ FALSE NEGATIVE RATE (FNR) ANALYSIS:\")\n",
    "print(f\"   Male FNR: {male_metrics['FNR']:.4f} ({male_metrics['FN']} missed of {male_metrics['FN'] + male_metrics['TP']} qualified)\")\n",
    "print(f\"   Female FNR: {female_metrics['FNR']:.4f} ({female_metrics['FN']} missed of {female_metrics['FN'] + female_metrics['TP']} qualified)\")\n",
    "print(f\"   Manual Difference: {manual_fnr_diff:.4f}\")\n",
    "print(f\"   Jurity Score: {fnr_difference:.4f}\")\n",
    "print(f\"   ‚úì Match: {abs(manual_fnr_diff - fnr_difference) < 0.001}\")\n",
    "print(f\"   Impact: {'Higher female FNR' if female_metrics['FNR'] > male_metrics['FNR'] else 'Higher male FNR' if male_metrics['FNR'] > female_metrics['FNR'] else 'Equal FNR'} (missing qualified talent)\")\n",
    "\n",
    "print(f\"\\nüìã FALSE OMISSION RATE (FOR) ANALYSIS:\")\n",
    "print(f\"   Male FOR: {male_metrics['FOR']:.4f} ({male_metrics['FN']} errors of {male_metrics['FN'] + male_metrics['TN']} negative predictions)\")\n",
    "print(f\"   Female FOR: {female_metrics['FOR']:.4f} ({female_metrics['FN']} errors of {female_metrics['FN'] + female_metrics['TN']} negative predictions)\")\n",
    "print(f\"   Manual Difference: {manual_for_diff:.4f}\")\n",
    "print(f\"   Jurity Score: {for_difference:.4f}\")\n",
    "print(f\"   ‚úì Match: {abs(manual_for_diff - for_difference) < 0.001}\")\n",
    "print(f\"   Impact: {'Higher female FOR' if female_metrics['FOR'] > male_metrics['FOR'] else 'Higher male FOR' if male_metrics['FOR'] > female_metrics['FOR'] else 'Equal FOR'} (unreliable rejections)\")\n",
    "\n",
    "print(f\"\\n‚ö†Ô∏è FALSE POSITIVE RATE (FPR) ANALYSIS:\")\n",
    "print(f\"   Male FPR: {male_metrics['FPR']:.4f} ({male_metrics['FP']} errors of {male_metrics['FP'] + male_metrics['TN']} negative cases)\")\n",
    "print(f\"   Female FPR: {female_metrics['FPR']:.4f} ({female_metrics['FP']} errors of {female_metrics['FP'] + female_metrics['TN']} negative cases)\")\n",
    "print(f\"   Manual Difference: {manual_fpr_diff:.4f}\")\n",
    "print(f\"   Jurity Score: {fpr_difference:.4f}\")\n",
    "print(f\"   ‚úì Match: {abs(manual_fpr_diff - fpr_difference) < 0.001}\")\n",
    "print(f\"   Impact: {'Higher female FPR' if female_metrics['FPR'] > male_metrics['FPR'] else 'Higher male FPR' if male_metrics['FPR'] > female_metrics['FPR'] else 'Equal FPR'} (false approvals)\")\n",
    "\n",
    "print(f\"\\nüìä PERFORMANCE GAP SUMMARY:\")\n",
    "print(f\"   Largest gap: {'FNR' if manual_fnr_diff >= max(manual_for_diff, manual_fpr_diff) else 'FOR' if manual_for_diff >= manual_fpr_diff else 'FPR'} difference ({max(manual_fnr_diff, manual_for_diff, manual_fpr_diff):.4f})\")\n",
    "print(f\"   Primary concern: {'Talent loss' if manual_fnr_diff >= max(manual_for_diff, manual_fpr_diff) else 'False rejections' if manual_for_diff >= manual_fpr_diff else 'False approvals'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive performance gap visualization dashboard\n",
    "fig, axes = plt.subplots(3, 3, figsize=(18, 16))\n",
    "fig.suptitle('Performance Gap Metrics: Diagnostic Analysis Dashboard', fontsize=16, fontweight='bold')\n",
    "\n",
    "# 1. Error Rates by Group Comparison\n",
    "groups = ['Male', 'Female']\n",
    "fnr_values = [male_metrics['FNR'], female_metrics['FNR']]\n",
    "for_values = [male_metrics['FOR'], female_metrics['FOR']]\n",
    "fpr_values = [male_metrics['FPR'], female_metrics['FPR']]\n",
    "\n",
    "x = np.arange(len(groups))\n",
    "width = 0.25\n",
    "\n",
    "axes[0,0].bar(x - width, fnr_values, width, label='FNR (Missing Qualified)', alpha=0.8, color='red')\n",
    "axes[0,0].bar(x, for_values, width, label='FOR (False Rejections)', alpha=0.8, color='orange')\n",
    "axes[0,0].bar(x + width, fpr_values, width, label='FPR (False Approvals)', alpha=0.8, color='purple')\n",
    "axes[0,0].set_xlabel('Demographic Group')\n",
    "axes[0,0].set_ylabel('Error Rate')\n",
    "axes[0,0].set_title('Error Rates by Group')\n",
    "axes[0,0].set_xticks(x)\n",
    "axes[0,0].set_xticklabels(groups)\n",
    "axes[0,0].legend()\n",
    "axes[0,0].grid(True, alpha=0.3)\n",
    "\n",
    "# 2. FNR Difference Visualization\n",
    "axes[0,1].bar(groups, fnr_values, color=['lightblue', 'lightcoral'], alpha=0.8)\n",
    "axes[0,1].set_ylabel('False Negative Rate')\n",
    "axes[0,1].set_title(f'FNR Difference: {manual_fnr_diff:.4f}\\n(Missing Qualified Candidates)')\n",
    "axes[0,1].grid(True, alpha=0.3)\n",
    "for i, v in enumerate(fnr_values):\n",
    "    axes[0,1].text(i, v + 0.005, f'{v:.3f}', ha='center', va='bottom')\n",
    "\n",
    "# 3. FOR Difference Visualization\n",
    "axes[0,2].bar(groups, for_values, color=['lightgreen', 'orange'], alpha=0.8)\n",
    "axes[0,2].set_ylabel('False Omission Rate')\n",
    "axes[0,2].set_title(f'FOR Difference: {manual_for_diff:.4f}\\n(Unreliable Rejections)')\n",
    "axes[0,2].grid(True, alpha=0.3)\n",
    "for i, v in enumerate(for_values):\n",
    "    axes[0,2].text(i, v + 0.002, f'{v:.3f}', ha='center', va='bottom')\n",
    "\n",
    "# 4. Performance Gap Scores Comparison\n",
    "gap_scores = [fnr_difference, for_difference, fpr_difference]\n",
    "gap_names = ['FNR\\nDifference', 'FOR\\nDifference', 'FPR\\nDifference']\n",
    "gap_colors = ['red', 'orange', 'purple']\n",
    "\n",
    "bars = axes[1,0].bar(gap_names, gap_scores, color=gap_colors, alpha=0.8)\n",
    "axes[1,0].set_ylabel('Performance Gap Score')\n",
    "axes[1,0].set_title('Performance Gap Metrics\\n(Lower = More Fair)')\n",
    "axes[1,0].grid(True, alpha=0.3)\n",
    "for bar, score in zip(bars, gap_scores):\n",
    "    axes[1,0].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.002,\n",
    "                  f'{score:.4f}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# 5. FPR Difference Visualization\n",
    "axes[1,1].bar(groups, fpr_values, color=['plum', 'gold'], alpha=0.8)\n",
    "axes[1,1].set_ylabel('False Positive Rate')\n",
    "axes[1,1].set_title(f'FPR Difference: {manual_fpr_diff:.4f}\\n(False Approvals)')\n",
    "axes[1,1].grid(True, alpha=0.3)\n",
    "for i, v in enumerate(fpr_values):\n",
    "    axes[1,1].text(i, v + 0.002, f'{v:.3f}', ha='center', va='bottom')\n",
    "\n",
    "# 6. Error Type Distribution\n",
    "error_types = ['False Negatives', 'False Positives']\n",
    "male_errors = [male_metrics['FN'], male_metrics['FP']]\n",
    "female_errors = [female_metrics['FN'], female_metrics['FP']]\n",
    "\n",
    "x = np.arange(len(error_types))\n",
    "width = 0.35\n",
    "\n",
    "axes[1,2].bar(x - width/2, male_errors, width, label='Male', alpha=0.8, color='lightblue')\n",
    "axes[1,2].bar(x + width/2, female_errors, width, label='Female', alpha=0.8, color='lightcoral')\n",
    "axes[1,2].set_ylabel('Number of Errors')\n",
    "axes[1,2].set_title('Error Counts by Type and Group')\n",
    "axes[1,2].set_xticks(x)\n",
    "axes[1,2].set_xticklabels(error_types)\n",
    "axes[1,2].legend()\n",
    "axes[1,2].grid(True, alpha=0.3)\n",
    "\n",
    "# 7. Diagnostic Assessment\n",
    "def assess_performance_gaps(fnr_diff, for_diff, fpr_diff):\n",
    "    \"\"\"Assess performance gap severity and provide diagnostic insights\"\"\"\n",
    "    assessments = []\n",
    "    \n",
    "    if fnr_diff > 0.1:\n",
    "        assessments.append(f\"üö® HIGH FNR GAP ({fnr_diff:.3f}) - Significant talent loss\")\n",
    "    elif fnr_diff > 0.05:\n",
    "        assessments.append(f\"‚ö†Ô∏è MODERATE FNR GAP ({fnr_diff:.3f}) - Monitor talent pipeline\")\n",
    "    else:\n",
    "        assessments.append(f\"‚úÖ LOW FNR GAP ({fnr_diff:.3f}) - Good qualified detection\")\n",
    "    \n",
    "    if for_diff > 0.1:\n",
    "        assessments.append(f\"üö® HIGH FOR GAP ({for_diff:.3f}) - Unreliable rejections\")\n",
    "    elif for_diff > 0.05:\n",
    "        assessments.append(f\"‚ö†Ô∏è MODERATE FOR GAP ({for_diff:.3f}) - Check rejection reliability\")\n",
    "    else:\n",
    "        assessments.append(f\"‚úÖ LOW FOR GAP ({for_diff:.3f}) - Reliable negative predictions\")\n",
    "    \n",
    "    if fpr_diff > 0.1:\n",
    "        assessments.append(f\"üö® HIGH FPR GAP ({fpr_diff:.3f}) - Inconsistent risk assessment\")\n",
    "    elif fpr_diff > 0.05:\n",
    "        assessments.append(f\"‚ö†Ô∏è MODERATE FPR GAP ({fpr_diff:.3f}) - Monitor false approvals\")\n",
    "    else:\n",
    "        assessments.append(f\"‚úÖ LOW FPR GAP ({fpr_diff:.3f}) - Consistent risk assessment\")\n",
    "    \n",
    "    return assessments\n",
    "\n",
    "gap_assessment = assess_performance_gaps(manual_fnr_diff, manual_for_diff, manual_fpr_diff)\n",
    "\n",
    "axes[2,0].axis('off')\n",
    "assessment_text = \"PERFORMANCE GAP ASSESSMENT:\\n\\n\" + \"\\n\\n\".join(gap_assessment)\n",
    "axes[2,0].text(0.05, 0.95, assessment_text, transform=axes[2,0].transAxes, \n",
    "              fontsize=10, verticalalignment='top', fontfamily='monospace',\n",
    "              bbox=dict(boxstyle=\"round,pad=0.5\", facecolor=\"lightgray\", alpha=0.8))\n",
    "\n",
    "# 8. Diagnostic Recommendations\n",
    "axes[2,1].axis('off')\n",
    "primary_issue = 'FNR' if manual_fnr_diff >= max(manual_for_diff, manual_fpr_diff) else 'FOR' if manual_for_diff >= manual_fpr_diff else 'FPR'\n",
    "\n",
    "recommendations = {\n",
    "    'FNR': [\n",
    "        \"PRIMARY ISSUE: Missing Qualified Talent\",\n",
    "        \"\",\n",
    "        \"FIXES:\",\n",
    "        \"‚Ä¢ Lower decision thresholds for affected group\",\n",
    "        \"‚Ä¢ Improve training data representation\",\n",
    "        \"‚Ä¢ Add features that better capture qualifications\",\n",
    "        \"‚Ä¢ Consider group-aware modeling\",\n",
    "        \"\",\n",
    "        \"MONITORING:\",\n",
    "        \"‚Ä¢ Track qualified candidate pipeline\",\n",
    "        \"‚Ä¢ Monitor hiring/promotion rates by group\"\n",
    "    ],\n",
    "    'FOR': [\n",
    "        \"PRIMARY ISSUE: Unreliable Rejections\",\n",
    "        \"\",\n",
    "        \"FIXES:\",\n",
    "        \"‚Ä¢ Calibrate prediction confidence scores\",\n",
    "        \"‚Ä¢ Adjust rejection criteria by group\",\n",
    "        \"‚Ä¢ Improve model training on negative cases\",\n",
    "        \"‚Ä¢ Review decision boundary placement\",\n",
    "        \"\",\n",
    "        \"MONITORING:\",\n",
    "        \"‚Ä¢ Track prediction reliability by group\",\n",
    "        \"‚Ä¢ Audit rejected applications regularly\"\n",
    "    ],\n",
    "    'FPR': [\n",
    "        \"PRIMARY ISSUE: Inconsistent Risk Assessment\",\n",
    "        \"\",\n",
    "        \"FIXES:\",\n",
    "        \"‚Ä¢ Tighten approval criteria for affected group\",\n",
    "        \"‚Ä¢ Improve negative case feature representation\",\n",
    "        \"‚Ä¢ Balance training data across groups\",\n",
    "        \"‚Ä¢ Review risk scoring methodology\",\n",
    "        \"\",\n",
    "        \"MONITORING:\",\n",
    "        \"‚Ä¢ Track approval success rates by group\",\n",
    "        \"‚Ä¢ Monitor downstream performance metrics\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "rec_text = \"\\n\".join(recommendations[primary_issue])\n",
    "axes[2,1].text(0.05, 0.95, rec_text, transform=axes[2,1].transAxes,\n",
    "              fontsize=9, verticalalignment='top', fontfamily='monospace',\n",
    "              bbox=dict(boxstyle=\"round,pad=0.5\", facecolor=\"lightyellow\", alpha=0.8))\n",
    "\n",
    "# 9. Implementation Action Plan\n",
    "axes[2,2].axis('off')\n",
    "action_plan = (\n",
    "    \"IMPLEMENTATION ACTION PLAN:\\n\\n\"\n",
    "    f\"1. IMMEDIATE (Week 1):\\n\"\n",
    "    f\"   Largest gap: {primary_issue} ({max(manual_fnr_diff, manual_for_diff, manual_fpr_diff):.3f})\\n\"\n",
    "    f\"   Focus: {'Talent retention' if primary_issue == 'FNR' else 'Prediction reliability' if primary_issue == 'FOR' else 'Risk consistency'}\\n\\n\"\n",
    "    f\"2. SHORT-TERM (Month 1):\\n\"\n",
    "    f\"   ‚Ä¢ Implement threshold adjustments\\n\"\n",
    "    f\"   ‚Ä¢ Begin model retraining\\n\"\n",
    "    f\"   ‚Ä¢ Set up monitoring dashboards\\n\\n\"\n",
    "    f\"3. LONG-TERM (Quarter 1):\\n\"\n",
    "    f\"   ‚Ä¢ Evaluate model architecture changes\\n\"\n",
    "    f\"   ‚Ä¢ Implement fairness-aware algorithms\\n\"\n",
    "    f\"   ‚Ä¢ Establish ongoing bias audit process\\n\\n\"\n",
    "    f\"SUCCESS METRICS:\\n\"\n",
    "    f\"   ‚Ä¢ All gaps < 0.05 (current max: {max(manual_fnr_diff, manual_for_diff, manual_fpr_diff):.3f})\\n\"\n",
    "    f\"   ‚Ä¢ Maintained or improved accuracy\\n\"\n",
    "    f\"   ‚Ä¢ Stakeholder satisfaction scores\"\n",
    ")\n",
    "axes[2,2].text(0.05, 0.95, action_plan, transform=axes[2,2].transAxes,\n",
    "              fontsize=9, verticalalignment='top', fontfamily='monospace',\n",
    "              bbox=dict(boxstyle=\"round,pad=0.5\", facecolor=\"lightblue\", alpha=0.8))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîß Performance Gap Diagnostic Framework\n",
    "\n",
    "### Understanding Your Results\n",
    "\n",
    "Performance Gap Metrics provide **actionable diagnostic information** about specific types of bias in your model. Unlike legal compliance or merit-based metrics that focus on overall fairness, performance gap metrics tell you exactly what type of error is causing unfairness.\n",
    "\n",
    "#### Key Diagnostic Questions:\n",
    "\n",
    "**High FNR Difference (False Negative Rate):**\n",
    "- *Question:* \"Is our model systematically missing qualified candidates from one demographic group?\"\n",
    "- *Business Impact:* Lost talent, reduced diversity in hiring/promotions\n",
    "- *Root Cause:* Often insufficient training data for underrepresented groups\n",
    "- *Fix Strategy:* Lower thresholds, improve data representation, feature engineering\n",
    "\n",
    "**High FOR Difference (False Omission Rate):**\n",
    "- *Question:* \"Are our negative predictions less reliable for one demographic group?\"\n",
    "- *Business Impact:* Unfair rejections, missed opportunities\n",
    "- *Root Cause:* Poor model calibration or biased training data\n",
    "- *Fix Strategy:* Calibrate prediction confidence, adjust decision boundaries\n",
    "\n",
    "**High FPR Difference (False Positive Rate):**\n",
    "- *Question:* \"Is our model giving unfair advantages to one demographic group?\"\n",
    "- *Business Impact:* Increased risk exposure, unfair resource allocation\n",
    "- *Root Cause:* Imbalanced representation of negative cases in training\n",
    "- *Fix Strategy:* Tighten criteria, improve negative case modeling\n",
    "\n",
    "---\n",
    "\n",
    "### Implementation Priority Matrix\n",
    "\n",
    "| Gap Size | Urgency | Action Required |\n",
    "|----------|---------|----------------|\n",
    "| **> 0.10** | üö® CRITICAL | Immediate model adjustment, halt deployment |\n",
    "| **0.05-0.10** | ‚ö†Ô∏è HIGH | Address within 30 days, enhanced monitoring |\n",
    "| **< 0.05** | ‚úÖ NORMAL | Routine monitoring, document for audits |\n",
    "\n",
    "### Business Value of Performance Gap Analysis\n",
    "\n",
    "1. **Precise Problem Identification:** Know exactly which error type causes bias\n",
    "2. **Targeted Solutions:** Apply specific fixes rather than general approaches\n",
    "3. **Resource Efficiency:** Focus improvement efforts where they matter most\n",
    "4. **Measurable Progress:** Track specific error reductions over time\n",
    "5. **Stakeholder Communication:** Explain bias issues in concrete, actionable terms\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ When to Use Performance Gap Metrics\n",
    "\n",
    "**Primary Use Cases:**\n",
    "- **Model Debugging:** When you know bias exists but need to understand the source\n",
    "- **Continuous Monitoring:** Ongoing detection of emerging bias patterns\n",
    "- **A/B Testing:** Comparing bias patterns across different model versions\n",
    "- **Stakeholder Reporting:** Communicating specific bias issues to technical teams\n",
    "\n",
    "**Integration with Other Metrics:**\n",
    "- **After Legal Compliance:** Use performance gaps to diagnose why legal metrics fail\n",
    "- **Before Merit-Based:** Understand error patterns before implementing merit-focused fairness\n",
    "- **With Business Metrics:** Connect error patterns to business outcomes\n",
    "\n",
    "**Performance Gap Metrics are your bias debugging toolkit** - use them to understand exactly what's wrong so you can fix it systematically."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}